from vllm import LLM, SamplingParams
from .agent_abc import AbstractLLMAgent
from .lesson_codegen_prompts import *
from .utils import *
import logging
import json
import jsonlines
from transformers import AutoTokenizer, AutoModel
from logging import Logger

class VLLMAgent(AbstractLLMAgent):
    def __init__(self, llm: LLM, model_name: str, logger: Logger, length_tokenizer: AutoTokenizer, temperature: int =0.2, reason_temperature: int=0.7, top_p : int =0.95, frequency_penalty : int =0):
        """
        llm: VLLM server instance
        model_name: huggingface model repository name for inference
        """
        # Call the parent class's __init__ method
        super().__init__(llm)
        print(f"Initializing VLLM agent {model_name}")
        #self.additional_param = additional_param
        self.model_name = model_name
        self.context = {}
        self.memory = []
        self.single_round_memory = []
        self.logger = logger
        self.length_tokenizer = length_tokenizer

        # NOTE: use config dictionary instead
        self.temperature=temperature
        self.top_p = top_p
        self.frequency_penalty = frequency_penalty

        self.reason_temperature = reason_temperature
    

    def generate_prompt(self, context: dict, task: str) -> str:
        """
        context: dictionary of context, the keys are:
            src_code: the source code yet to optimize
            tgt_code: the code optimized from source code by LLM
            feedback: execution feedback in the terminal
            lessons: the explanation generated by LLM
            issues: the issues of explanation identified by LLM

        task: str, has to be "CODE", "LESSON", "IDENTIFY" or "MODIFY, choose which prompt to generate

        return: generated prompt for different tasks
        """
        if task == "CODE":
            if context["lessons"] == "":
                prompt = generate_python_code(func=context["func"])
            else:
                print("generate code with lessons: ")
                prompt = generate_python_code_with_lessons(func=context["func"], lessons=context["lessons"])
        elif task == "LESSON":
            prompt = generate_lesson_partial_correct_code(gen_code=context["code"], 
                                                          total_test_cases=context["total_test_cases_num"], 
                                                          passed_test_cases=context["passed_test_cases_num"],
                                                          failed_test_cases=context["failed_test_cases_num"])
        else:
            print(f"Not valid task {task}, such prompt is not valid.")
        
        
        return prompt


    def process_response(self, response) -> str:
        return clean_output(response)
    
    def infer(self, prompt: str) -> str:
        """
        Inference the LLM with vllm OpenAI API and vllm server 

        Args:
            prompt: the prompt that to use inference LLM.

        return: the response from LLM
        """

        in_tokens = len(self.length_tokenizer.tokenize(prompt))
        output = self.llm.chat.completions.create(
                model=self.model_name,
                messages=[
                {"role": "user", "content": prompt}
                ],
                temperature=self.temperature,
                top_p=self.top_p,
                frequency_penalty=self.frequency_penalty,
                #truncate=True
            )

        output = output.choices[0].message.content
        out_tokens = len(self.length_tokenizer.tokenize(output))
        #optimized_code = clean_output(optimized_code)
        return output, in_tokens, out_tokens
    
    def reason(self, prompt: str, temperature: float, top_p: float, frequency_penalty: float, max_completion_tokens: int=250) -> str:
        """
        Inference the LLM with vllm OpenAI API and vllm server, and a different set of temperature for reasoning

        Args:
            prompt: the prompt that to use inference LLM.

        return: the response from LLM
        """
        in_tokens = len(self.length_tokenizer.tokenize(prompt))

        output = self.llm.chat.completions.create(
                model=self.model_name,
                messages=[
                {"role": "user", "content": prompt}
                ],
                temperature=temperature,
                top_p=top_p,
                frequency_penalty=frequency_penalty,
                max_completion_tokens=max_completion_tokens,
                #truncate=True
            )

        output = output.choices[0].message.content
        out_tokens = len(self.length_tokenizer.tokenize(output))

        #optimized_code = clean_output(optimized_code)
        return output, in_tokens, out_tokens
    
    def generate_code(self, context: dict) -> str:
        """
        Perform Code Optimization by using LLM

        Args:
            context (Dict[any, any]): dictionary, containing necessary information to form a prompt

        Returns
            str: optimized code 
        """
        print("generating: ", self.model_name)
        prompt = self.generate_prompt(context=context, task="CODE")
        #self.logger.info("Optimizer Prompt: \n%s", prompt)
        #self.logger.info("OPTIMIZER PROMPT END!!!")
        output, in_tokens, out_tokens = self.infer(prompt=prompt)
        code = clean_output(output)
        if code == "":
            print(f"No code block found for {self.model_name}")
        return code, in_tokens, out_tokens


    def generate_lesson(self, context: dict) -> str:
        """
        Generate Lesson by using LLM

        Args:
            context: dictionary, containing necessary information to form a prompt

        Returns
            str: a lesson generated by LLM
        """

        prompt = self.generate_prompt(context=context, task="LESSON")
        #self.logger.debug("Lesson Prompt: \n%s", prompt)
        #self.logger.debug("Finished Lesson Prompt Logging.\n")
        #self.logger.info("Lesson Prompt: \n%s", prompt)
        output, in_tokens, out_tokens = self.reason(prompt=prompt, temperature=self.reason_temperature, top_p=0.95, frequency_penalty=0.5)
        #optimized_code = clean_output(output)
        
        return output, in_tokens, out_tokens

    
    def save_memory(self):
        save_name = f"{self.model_name}_memory.jsonl"
        save_name = save_name.split('/')[-1]
        # Write to JSON Lines file
        with jsonlines.open(save_name, mode="w") as writer:
            for round in self.memory:
                writer.write_all(round)