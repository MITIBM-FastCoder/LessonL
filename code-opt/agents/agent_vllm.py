from vllm import LLM, SamplingParams
from .agent_abc import AbstractLLMAgent
from .prompts.lessonl_codeopt_prompts import *
from .utils import *
import logging
import json
import jsonlines
from transformers import AutoTokenizer, AutoModel
from logging import Logger

class VLLMAgent(AbstractLLMAgent):
    def __init__(self, llm: LLM, model_name: str, logger: Logger, length_tokenizer: AutoTokenizer, additional_package: str, temperature: int =0.2, reason_temperature: int=0.2, top_p : int =0.95, frequency_penalty : int =0):
        """
        llm: VLLM server instance
        model_name: huggingface model repository name for inference
        """
        # Call the parent class's __init__ method
        super().__init__(llm)
        print(f"Initializing VLLM agent {model_name}")
        self.model_name = model_name
        self.context = {}
        self.memory = []
        self.single_round_memory = []
        self.logger = logger
        self.length_tokenizer = length_tokenizer
        self.additional_package = additional_package

        # NOTE: use config dictionary instead
        self.temperature=temperature
        self.top_p = top_p
        self.frequency_penalty = frequency_penalty

        self.reason_temperature = reason_temperature
    

    def generate_prompt(self, context: dict, task: str) -> str:
        """
        context: dictionary of context, the keys are:
            src_code: the source code yet to optimize
            tgt_code: the code optimized from source code by LLM
            feedback: execution feedback in the terminal
            lessons: the explanation generated by LLM
            issues: the issues of explanation identified by LLM

        task: str, has to be "CODE" and "LESSON", choose which prompt to generate

        return: generated prompt for different tasks
        """
        if task == "CODE":
            #
            if context["lessons"] == "":
                prompt = generate_code_opt_prompt_code(src_code=context["src_code"], additional_package=self.additional_package)
            else:
                prompt = generate_code_opt_prompt_code_with_lessons(src_code=context["src_code"], lessons=context["lessons"], additional_package=self.additional_package)
        elif task == "LESSON":
            log, tag, speedup = process_execution_feedback(log=context["feedback"])
            if tag == "CORRECT":
                if speedup >= 1.0 and speedup < 1.1:
                    faster_or_slower = "slightly faster"
                elif speedup >= 1.1:
                    faster_or_slower = "significantly faster"
                else:
                    faster_or_slower = "slower"
                prompt = generate_lesson_correct_tgt_code_prompt(src_code=context["src_code"], tgt_code=context["tgt_code"], faster_or_slower=faster_or_slower, speedup=speedup)
            elif tag == "INCORRECT":
                prompt = generate_lesson_incorrect_tgt_code_prompt(src_code=context["src_code"], tgt_code=context["tgt_code"])
            elif tag == "NOT_COMPILABLE":
                log_lines = log.splitlines()  # Split log into lines
                log = "\n".join(log_lines[:7]) if len(log_lines) > 7 else "\n".join(log_lines) # Only first 7 lines of error log because of 4k context window constraints
                prompt = generate_lesson_not_compilable_tgt_code_prompt(src_code=context["src_code"], tgt_code=context["tgt_code"], feedback=log)
        else:
            print(f"Not valid task {task}, such prompt is not valid.")
        
        return prompt


    def process_response(self, response) -> str:
        return clean_output(response)
    
    def infer(self, prompt: str) -> str:
        """
        Inference the LLM with vllm OpenAI API and vllm server 

        Args:
            prompt: the prompt that to use inference LLM.

        return: the response from LLM
        """

        in_tokens = len(self.length_tokenizer.tokenize(prompt))
        output = self.llm.chat.completions.create(
                model=self.model_name,
                messages=[
                {"role": "user", "content": prompt}
                ],
                temperature=self.temperature,
                top_p=self.top_p,
                frequency_penalty=self.frequency_penalty
            )

        output = output.choices[0].message.content
        out_tokens = len(self.length_tokenizer.tokenize(output))
        #optimized_code = clean_output(optimized_code)
        return output, in_tokens, out_tokens
    
    def reason(self, prompt: str, temperature: float, top_p: float, frequency_penalty: float, max_completion_tokens: int=250) -> str:
        """
        Inference the LLM with vllm OpenAI API and vllm server, and a different set of temperature for reasoning

        Args:
            prompt: the prompt that to use inference LLM.

        return: the response from LLM
        """
        in_tokens = len(self.length_tokenizer.tokenize(prompt))

        output = self.llm.chat.completions.create(
                model=self.model_name,
                messages=[
                {"role": "user", "content": prompt}
                ],
                temperature=temperature,
                top_p=top_p,
                frequency_penalty=frequency_penalty,
                max_completion_tokens=max_completion_tokens
            )

        output = output.choices[0].message.content
        out_tokens = len(self.length_tokenizer.tokenize(output))

        #optimized_code = clean_output(optimized_code)
        return output, in_tokens, out_tokens
    
    def optimize_code(self, context: dict) -> str:
        """
        Perform Code Optimization by using LLM

        Args:
            context (Dict[any, any]): dictionary, containing necessary information to form a prompt

        Returns
            str: optimized code 
        """
        prompt = self.generate_prompt(context=context, task="CODE")
        #self.logger.info("Optimizer Prompt: \n%s", prompt)
        #self.logger.info("OPTIMIZER PROMPT END!!!")
        output, in_tokens, out_tokens = self.infer(prompt=prompt)
        # print(f"Agent {self.model_name} Optimizer output: {output}")
        optimized_code = clean_output(output)
        if optimized_code == "":
            # Hack: sometimes the model does not follow the instructions to output code block. We try to catch cases with both "c++" and "cpp" in the output
            optimized_code = clean_output(output, language="cpp")
            if optimized_code == "":
                print(f"No code block found for {self.model_name}")
        return optimized_code, in_tokens, out_tokens
    
    


    def generate_lesson(self, context: dict) -> str:
        """
        Generate Lesson by using LLM

        Args:
            context: dictionary, containing necessary information to form a prompt

        Returns
            str: a lesson generated by LLM
        """

        prompt = self.generate_prompt(context=context, task="LESSON")
        #self.logger.debug("Lesson Prompt: \n%s", prompt)
        #self.logger.debug("Finished Lesson Prompt Logging.\n")
        #self.logger.info("Lesson Prompt: \n%s", prompt)
        output, in_tokens, out_tokens = self.reason(prompt=prompt, temperature=self.reason_temperature, top_p=0.95, frequency_penalty=0.5)
        #optimized_code = clean_output(output)
        
        return output, in_tokens, out_tokens
    
    
    def save_memory(self):
        save_name = f"{self.model_name}_memory.jsonl"
        save_name = save_name.split('/')[-1]
        # Write to JSON Lines file
        with jsonlines.open(save_name, mode="w") as writer:
            for round in self.memory:
                writer.write_all(round)